# Projet Pipeline ELT

ğŸŒ **Contexte**  
Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du module "Data Engineering" Ã  HETIC (promo DIA2). Il consiste Ã  crÃ©er un pipeline ELT complet de lâ€™ingestion Ã  la modÃ©lisation, en utilisant un dataset libre sur les voitures.

ğŸ“š **Objectifs du projet**  
- Ingestion de donnÃ©es brutes Ã  partir dâ€™un fichier CSV
- Stockage dans PostgreSQL via Docker
- Transformation et modÃ©lisation des donnÃ©es en base
- Orchestration automatique du pipeline
- Documentation claire et livrables prÃªts pour soutenance

âš™ï¸ **Technologies utilisÃ©es**  
- **Python** : ingestion, orchestration
- **PostgreSQL** : base de donnÃ©es relationnelle
- **Docker** : environnement de base de donnÃ©es isolÃ©
- **psycopg2-binary / pandas / sqlalchemy** : connexion et manipulation de donnÃ©es
- **SQL** : crÃ©ation des tables et transformations
- **dotenv / Makefile / logging / time** : gestion dâ€™environnement, automatisation, journalisation

## ğŸ“ **Installation du projet**

### 1. Cloner le repo
```bash
git clone https://github.com/votre-utilisateur/Projet-Pipeline-ELT.git
cd Projet-Pipeline-ELT
```

### 2. Initialiser lâ€™environnement
```bash
cp .env.example .env
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Lancer PostgreSQL avec Docker
```bash
docker compose up -d
```

---

## ğŸ“ Utilisation du pipeline

### 1. Ingestion simple (brute)
```bash
python scripts/extract_load.py
```

### 2. Orchestration complÃ¨te (ingestion + transformations)
```bash
python scripts/orchestrator.py
```

### 3. VÃ©rification des donnÃ©es aprÃ¨s transformation:
```bash
psql -d cars_project -c "SELECT * FROM fact_car_sales LIMIT 10;"
```

### 4. Ou via le Makefile :
```bash
make run       # Lance docker
make etl       # Lance le pipeline complet
make stop      # Stoppe les conteneurs
make reset     # Stoppe tout + supprime les volumes
```

Un rapport automatique est gÃ©nÃ©rÃ© dans `logs/report.txt` Ã  chaque exÃ©cution.

---

## ğŸ“Š Structure du projet

```
Projet-Pipeline-ELT/
â”œâ”€â”€ data/raw/                # DonnÃ©es sources (non versionnÃ©es)
â”œâ”€â”€ docs/                    # SchÃ©mas, prÃ©sentation, PPT
â”œâ”€â”€ scripts/                 # Scripts Python : ingestion + orchestration
â”œâ”€â”€ sql/                    # CrÃ©ation des tables et transformations
â”œâ”€â”€ logs/                   # Logs d'exÃ©cution et rapports
â”œâ”€â”€ .env.example            # Variables dâ€™environnement (exemple)
â”œâ”€â”€ docker-compose.yml      # Conteneur PostgreSQL
â”œâ”€â”€ Makefile                # Automatisation
â”œâ”€â”€ requirements.txt        # Packages Python
â””â”€â”€ README.md               # Ce fichier
```

---

##ModÃ©lisation des donnÃ©es

Le modÃ¨le des donnÃ©es utilisÃ© dans ce projet suit une structure en Ã©toile avec des tables dimensionnelles et une table des faits. Voici un aperÃ§u des tables et de leurs relations :

```
dim_make : Table contenant les marques de voitures

make_id : ClÃ© primaire

make_name : Nom de la marque (ex : BMW, Audi, etc.)

dim_model : Table contenant les modÃ¨les de voitures

model_id : ClÃ© primaire

model_name : Nom du modÃ¨le (ex : X5, A4, etc.)

make_id : ClÃ© Ã©trangÃ¨re, rÃ©fÃ©rence Ã  dim_make

dim_transmission : Table contenant les types de transmission

transmission_id : ClÃ© primaire

transmission_type : Type de transmission (ex : Manual, Automatic)

dim_fuel : Table contenant les types de carburant

fuel_id : ClÃ© primaire

fuel_type : Type de carburant (ex : Petrol, Diesel)

fact_car_sales : Table de faits qui relie les dimensions et contient les mesures

sale_id : ClÃ© primaire

make_id : ClÃ© Ã©trangÃ¨re, rÃ©fÃ©rence Ã  dim_make

model_id : ClÃ© Ã©trangÃ¨re, rÃ©fÃ©rence Ã  dim_model

transmission_id : ClÃ© Ã©trangÃ¨re, rÃ©fÃ©rence Ã  dim_transmission

fuel_id : ClÃ© Ã©trangÃ¨re, rÃ©fÃ©rence Ã  dim_fuel

year, price, mileage, tax, mpg, engine_size : Mesures des ventes
```

Les relations entre ces tables permettent d'analyser les ventes de voitures par marque, modÃ¨le, transmission, carburant, et autres critÃ¨res. Un schÃ©ma ER de la base de donnÃ©es est disponible dans le dossier docs/.

---

## ğŸ§° Equipe

- **GrÃ©gory** : Orchestration & Ingestion
- **Othmane** : ModÃ©lisation & SQL
- **Lucien** : Documentation & PrÃ©sentation

---

## âœ… Livrables attendus

- [x] Code source (Python, SQL) via GitHub
- [x] Fichier `.env.example` et `README.md`
- [x] SchÃ©ma du pipeline (`docs/schema_pipeline.png`)
- [x] SchÃ©ma BDD (`docs/schema_bdd.png`)
- [x] `presentation.pptx` pour soutenance (max 10 min)

---

## ğŸ”’ Notes

- Ne pas versionner `.env` ni `venv/`
- Docker doit Ãªtre configurÃ© avec WSL2 sur chaque poste
- PostgreSQL tourne sur `localhost:5432` par dÃ©faut
- VÃ©rifier les ports et configurations dans `docker-compose.yml`
- Les logs sont gÃ©nÃ©rÃ©s dans `logs/` pour chaque exÃ©cution

---

## Conclusion
Voici un README.md complet qui couvre tous les aspects du projet : installation, utilisation, structure, technologies, et documentation technique pour la modÃ©lisation.
